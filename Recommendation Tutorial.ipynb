{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MovieLens recommendation tutorial\n",
    "Today we are going to build on what we learned during the course, and build several types of recommender systems for the movielens dataset, you can download it right [here](https://grouplens.org/datasets/movielens/1m/).\n",
    "MovieLens is a web site that helps people find movies to watch. It has hundreds of thousands of registered users. And we are going to use this dataset with 1M ratings from 6000 users on 4000 movies (it was released in 2003 so it is quite old, but there are newer versions of this dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data preprocessing\n",
    "First we are going just to preprocess the data a bit to get it in a clear manner, and replace some of the column values by clearer values, nothing too fancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "random.seed(42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define directories\n",
    "MOVIELENS_DIR = 'ml-1m'\n",
    "USER_DATA_FILE = 'users.dat'\n",
    "MOVIE_DATA_FILE = 'movies.dat'\n",
    "RATING_DATA_FILE = 'ratings.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specify User's Age and Occupation Column\n",
    "AGES = { 1: \"Under 18\", 18: \"18-24\", 25: \"25-34\", 35: \"35-44\", 45: \"45-49\", 50: \"50-55\", 56: \"56+\" }\n",
    "OCCUPATIONS = { 0: \"other or not specified\", 1: \"academic/educator\", 2: \"artist\", 3: \"clerical/admin\",\n",
    "                4: \"college/grad student\", 5: \"customer service\", 6: \"doctor/health care\",\n",
    "                7: \"executive/managerial\", 8: \"farmer\", 9: \"homemaker\", 10: \"K-12 student\", 11: \"lawyer\",\n",
    "                12: \"programmer\", 13: \"retired\", 14: \"sales/marketing\", 15: \"scientist\", 16: \"self-employed\",\n",
    "                17: \"technician/engineer\", 18: \"tradesman/craftsman\", 19: \"unemployed\", 20: \"writer\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define csv files to be saved into\n",
    "USERS_CSV_FILE = 'users.csv'\n",
    "MOVIES_CSV_FILE = 'movies.csv'\n",
    "RATINGS_CSV_FILE = 'ratings.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Ratings File\n",
    "ratings = pd.read_csv(os.path.join(MOVIELENS_DIR, RATING_DATA_FILE), \n",
    "                    sep='::', \n",
    "                    engine='python', \n",
    "                    encoding='latin-1',\n",
    "                    names=['user_id', 'movie_id', 'rating', 'timestamp'])\n",
    "\n",
    "# Set max_userid to the maximum user_id in the ratings\n",
    "max_userid = ratings['user_id'].drop_duplicates().max()\n",
    "# Set max_movieid to the maximum movie_id in the ratings\n",
    "max_movieid = ratings['movie_id'].drop_duplicates().max()\n",
    "\n",
    "print(len(ratings), 'ratings loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just check what it looks like\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save into ratings.csv\n",
    "ratings.to_csv(RATINGS_CSV_FILE, \n",
    "               sep='\\t', \n",
    "               header=True,\n",
    "               index=False,\n",
    "               encoding='latin-1')\n",
    "print('Saved to', RATINGS_CSV_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Users File\n",
    "users = pd.read_csv(os.path.join(MOVIELENS_DIR, USER_DATA_FILE), \n",
    "                    sep='::', \n",
    "                    engine='python', \n",
    "                    encoding='latin-1',\n",
    "                    names=['user_id', 'gender', 'age', 'occupation', 'zipcode'])\n",
    "users['age_desc'] = users['age'].apply(lambda x: AGES[x])\n",
    "users['occ_desc'] = users['occupation'].apply(lambda x: OCCUPATIONS[x])\n",
    "\n",
    "print(len(users), 'descriptions of', max_userid, 'users loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save into users.csv\n",
    "users.to_csv(USERS_CSV_FILE, \n",
    "             sep='\\t', \n",
    "             header=True,\n",
    "             index=False,\n",
    "             encoding='latin-1')\n",
    "print('Saved to', USERS_CSV_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Movies File\n",
    "movies = pd.read_csv(os.path.join(MOVIELENS_DIR, MOVIE_DATA_FILE), \n",
    "                    sep='::', \n",
    "                    engine='python', \n",
    "                    encoding='latin-1',\n",
    "                    names=['movie_id', 'title', 'genres'])\n",
    "print(len(movies), 'descriptions of', max_movieid, 'movies loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save into movies.csv\n",
    "movies.to_csv(MOVIES_CSV_FILE, \n",
    "              sep='\\t', \n",
    "              header=True,\n",
    "              index=False)\n",
    "print('Saved to', MOVIES_CSV_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Movie Recommendation with Content-Based and Collaborative Filtering\n",
    "“*What movie should I watch this evening?*” \n",
    "\n",
    "Have you ever had to answer this question at least once when you came home from work? As for me — yes, and more than once. From Netflix to Hulu, the need to build robust movie recommendation systems is extremely important given the huge demand for personalized content of modern consumers.\n",
    "\n",
    "An example of recommendation system is such as this:\n",
    "* User A watches **Game of Thrones** and **Breaking Bad**.\n",
    "* User B does search on **Game of Thrones**, then the system suggests **Breaking Bad** from data collected about user A.\n",
    "\n",
    "Recommendation systems are used not only for movies, but on multiple other products and services like Amazon (Books, Items), Pandora/Spotify (Music), Google (News, Search), YouTube (Videos) etc.\n",
    "\n",
    "Two most ubiquitous types of personalized recommendation systems are **Content-Based** and **Collaborative Filtering**. Collaborative filtering produces recommendations based on the knowledge of users’ attitude to items, that is it uses the “wisdom of the crowd” to recommend items. In contrast, content-based recommendation systems focus on the attributes of the items and give you recommendations based on the similarity between them.\n",
    "\n",
    "In this notebook, we will attempt at implementing these two systems to recommend movies and evaluate them to see which one performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "Let's load this data into Python. we will load the dataset with Pandas onto Dataframes **ratings**, **users**, and **movies**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reading ratings file\n",
    "# Ignore the timestamp column\n",
    "ratings = pd.read_csv('ratings.csv', sep='\\t', encoding='latin-1')\n",
    "\n",
    "# Reading users file\n",
    "users = pd.read_csv('users.csv', sep='\\t', encoding='latin-1')\n",
    "\n",
    "# Reading movies file\n",
    "movies = pd.read_csv('movies.csv', sep='\\t', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take a peak into the content of each file to understand them better.\n",
    "\n",
    "### Ratings Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms that there are approximately 1M ratings for different user and movie combinations.\n",
    "\n",
    "### Users Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms that there are 6040 users and we have 5 features for each (unique user ID, gender, age, occupation and the zip code they are living in).\n",
    "\n",
    "### Movies Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains attributes of the 3883 movies. There are 3 columns including the movie ID, their titles, and their genres. Genres are pipe-separated and are selected from 18 genres (Action, Adventure, Animation, Children's, Comedy, Crime, Documentary, Drama, Fantasy, Film-Noir, Horror, Musical, Mystery, Romance, Sci-Fi, Thriller, War, Western)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "### Titles\n",
    "Are there certain words that feature more often in Movie Titles? Let's try out a nice visualization using word clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Create a wordcloud of the movie titles\n",
    "movies['title'] = movies['title'].fillna(\"\").astype('str')\n",
    "# we just join all the titles together into a big string\n",
    "title_corpus = ' '.join(movies['title'])\n",
    "\n",
    "title_wordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', height=2000, width=4000).generate(title_corpus)\n",
    "\n",
    "# Plot the wordcloud\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(title_wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok maybe not the most interesting visualization in the world, but gives us an idea of what words are in the movie titles. We can see Day, Man, Love ...\n",
    "\n",
    "### Ratings\n",
    "Now let's examine the ratings a bit more. Some descriptive statistics first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings['rating'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that users are quite generous in their ratings. The mean rating is 3.58 on a scale of 5. Half the movies have a rating of 4 and 5. But because different people might rate on different scales, we can't really conclude.\n",
    "\n",
    "Let's also take a look at a subset of 20 movies with the highest rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all 3 files into one dataframe\n",
    "dataset = pd.merge(pd.merge(movies, ratings),users)\n",
    "\n",
    "# Display 20 movies with highest ratings\n",
    "dataset[['title','genres','rating']].sort_values('rating', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genres\n",
    "The genres variable will surely be important while building the recommendation engines since it describes the content of the film (i.e. Animation, Horror, Sci-Fi). A basic assumption is that films in the same genre should have similar contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "genre_counter = Counter([genre for row in movies['genres'].apply(lambda s: s.split('|')).values for genre in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_counter.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 5 genres are, in that respect order: Drama, Comedy, Action, Thriller, and Romance. Let's visualize them in a wordcloud again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the wordcloud\n",
    "genre_wordcloud = WordCloud(width=1000,height=400, background_color='white')\n",
    "genre_wordcloud.generate_from_frequencies(genre_counter)\n",
    "\n",
    "# Plot the wordcloud\n",
    "f, ax = plt.subplots(figsize=(16, 8))\n",
    "plt.imshow(genre_wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content based collaborative filtering\n",
    "Like we said in the course, the metadata attached to an item can be used to compute similarities, and recommend to a user similar items to what he saw.\n",
    "We saw briefly how can we use TF-IDF to compute item vectors from descriptions, categories, ... and then use the similarity between item vectors (cosine similarity for instance).\n",
    "We are going to do that here with movies genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Break up the big genre string into a string array\n",
    "movies['genres'] = movies['genres'].str.split('|')\n",
    "# Convert genres to string value\n",
    "movies['genres'] = movies['genres'].fillna(\"\").astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf = TfidfVectorizer(analyzer='word', min_df=0)\n",
    "tfidf_matrix = tf.fit_transform(movies['genres'])\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "cosine_sim[:4, :4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a pairwise cosine similarity matrix for all the movies in the dataset. The next step is to write a function that returns the most similar movies based on the cosine similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a 1-dimensional array with movie titles\n",
    "titles = movies['title']\n",
    "indices = pd.Series(movies.index, index=movies['title'])\n",
    "\n",
    "# Function that get movie recommendations based on the cosine similarity score of movie genres\n",
    "def recommendations_from_similarities(sims, title, k=10):\n",
    "    idx = indices[title]\n",
    "    sim_scores = list(enumerate(sims[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[:k+1]\n",
    "    # the most similar movie to a given one will always be itself, so we skip that one\n",
    "    movie_indices = [i[0] for i in sim_scores if i[0] != idx]\n",
    "    return titles.iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_from_similarities(cosine_sim, 'Matrix, The (1999)').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_from_similarities(cosine_sim, 'Toy Story (1995)').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_from_similarities(cosine_sim, 'Saving Private Ryan (1998)').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering\n",
    "Now we are going to apply what we learned about item-item and user-user collaborative filtering.\n",
    "\n",
    "First we need to split our data to test and train datasets, so we can evaluate what we are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill NaN values in user_id and movie_id column with 0\n",
    "ratings['user_id'] = ratings['user_id'].fillna(0)\n",
    "ratings['movie_id'] = ratings['movie_id'].fillna(0)\n",
    "\n",
    "# Replace NaN values in rating column with average of all values\n",
    "ratings['rating'] = ratings['rating'].fillna(ratings['rating'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into 80% train and 20% test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(ratings, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def to_sparse_matrix(df, rating_col=\"rating\"):\n",
    "    return scipy.sparse.csr_matrix((df[rating_col], (df['user_id'], df['movie_id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sparse = to_sparse_matrix(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# User Similarity Matrix\n",
    "user_sim = 1 - pairwise_distances(train_sparse, metric='cosine')\n",
    "user_sim[np.isnan(user_sim)] = 0\n",
    "\n",
    "print(user_sim[:4, :4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item Similarity Matrix\n",
    "\n",
    "item_sim = 1 - pairwise_distances(train_sparse.T, metric='cosine')\n",
    "item_sim[np.isnan(item_sim)] = 0\n",
    "\n",
    "print(item_sim[:4, :4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_from_similarities(item_sim, 'Matrix, The (1999)').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_from_similarities(item_sim, 'Toy Story (1995)').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_from_similarities(item_sim, 'Saving Private Ryan (1998)').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted cosine similarity\n",
    "# We need to substract average rating for each item from its rating\n",
    "train_data_adj = train_data.join(train_data.groupby('movie_id')['rating'].mean(), on='movie_id', rsuffix='_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_adj[\"adjusted_rating\"] = train_data_adj[\"rating\"] - train_data_adj[\"rating_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_adj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_adj_sparse = to_sparse_matrix(train_data_adj, rating_col=\"adjusted_rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item Similarity Matrix\n",
    "\n",
    "item_sim_adj = 1 - pairwise_distances(train_adj_sparse.T, metric='cosine')\n",
    "item_sim_adj[np.isnan(item_sim_adj)] = 0\n",
    "\n",
    "print(item_sim_adj[:4, :4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recommendations_from_similarities(item_sim_adj, 'Saving Private Ryan (1998)').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the prediction functions\n",
    "Now we are going to write a couple of functions that will predict ratings for user, item pairs in the test set.\n",
    "We will be using the formulas from the course, to take the weighted average of ratings from:\n",
    "* The items that the user already rated, weighted by similarity to the target item\n",
    "* The ratings that the other users gave to the item, weighted by the similarity with the target user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "def predict_user_user_cf(train_ratings, test_ratings, similarity_matrix):\n",
    "    mean_user_ratings = train_data.groupby('user_id')['rating'].mean()\n",
    "    \n",
    "    train_ratings = train_ratings.join(mean_user_ratings, on='user_id', rsuffix='_user_mean')\n",
    "    test_ratings = test_ratings.join(mean_user_ratings, on='user_id', rsuffix='_user_mean')\n",
    "    \n",
    "    train_ratings[\"ratings_adjusted_user_mean\"] = train_ratings[\"rating\"] - train_ratings[\"rating_user_mean\"]\n",
    "    train_ratings_sparse = to_sparse_matrix(train_ratings, \"ratings_adjusted_user_mean\")\n",
    "    \n",
    "    ratings_weighted_by_user_sim = sparse.csr_matrix.dot(similarity_matrix, train_ratings_sparse) / np.sum(similarity_matrix, axis=1, keepdims=True)\n",
    "    \n",
    "    indices = np.array([[user_id, movie_id] for user_id in range(ratings_weighted_by_user_sim.shape[0]) for movie_id in range(ratings_weighted_by_user_sim.shape[1])])\n",
    "    predicted_ratings = ratings_weighted_by_user_sim.flatten()\n",
    "    \n",
    "    predicted_ratings_df = pd.DataFrame({\n",
    "        \"user_id\": indices[:, 0],\n",
    "        \"movie_id\": indices[:, 1],\n",
    "        \"predicted_rating\": predicted_ratings,\n",
    "    })\n",
    "    \n",
    "    predicted_ratings_df = pd.merge(predicted_ratings_df, test_ratings, on=[\"user_id\", \"movie_id\"])\n",
    "    predicted_ratings_df[\"predicted_rating\"] = predicted_ratings_df[\"predicted_rating\"] + predicted_ratings_df[\"rating_user_mean\"]\n",
    "    return predicted_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_user_predictions = predict_user_user_cf(train_data, test_data, user_sim)\n",
    "user_user_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_item_item_cf(train_ratings, test_ratings, similarity_matrix):\n",
    "    mean_item_ratings = train_data.groupby('movie_id')['rating'].mean()\n",
    "    \n",
    "    train_ratings = train_ratings.join(mean_item_ratings, on='movie_id', rsuffix='_item_mean')\n",
    "    test_ratings = test_ratings.join(mean_item_ratings, on='movie_id', rsuffix='_item_mean')\n",
    "    \n",
    "    train_ratings[\"ratings_adjusted_item_mean\"] = train_ratings[\"rating\"] - train_ratings[\"rating_item_mean\"]\n",
    "    train_ratings_sparse = to_sparse_matrix(train_ratings, \"ratings_adjusted_item_mean\")\n",
    "    \n",
    "    ratings_weighted_by_item_sim = sparse.csr_matrix.dot(train_ratings_sparse, similarity_matrix) / np.sum(similarity_matrix, axis=0, keepdims=True)\n",
    "    \n",
    "    indices = np.array([[user_id, movie_id] for user_id in range(ratings_weighted_by_item_sim.shape[0]) for movie_id in range(ratings_weighted_by_item_sim.shape[1])])\n",
    "    predicted_ratings = ratings_weighted_by_item_sim.flatten()\n",
    "    \n",
    "    predicted_ratings_df = pd.DataFrame({\n",
    "        \"user_id\": indices[:, 0],\n",
    "        \"movie_id\": indices[:, 1],\n",
    "        \"predicted_rating\": predicted_ratings,\n",
    "    })\n",
    "    \n",
    "    predicted_ratings_df = pd.merge(predicted_ratings_df, test_ratings, on=[\"user_id\", \"movie_id\"])\n",
    "    predicted_ratings_df[\"predicted_rating\"] = predicted_ratings_df[\"predicted_rating\"] + predicted_ratings_df[\"rating_item_mean\"]\n",
    "    return predicted_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_item_predictions = predict_item_item_cf(train_data, test_data, item_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are just going to replace nans here by the average predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_item_predictions['predicted_rating'] = item_item_predictions['predicted_rating'].fillna(item_item_predictions['predicted_rating'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "We are going to evaluate our methods on the test set, using the mean absolute error metric.\n",
    "Which is as follows:\n",
    "$$\\mathit{MAE} =\\sqrt{\\frac{1}{N} \\sum |x_i -\\hat{x_i}|}$$\n",
    "\n",
    "This just measures the average deviation from the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAPE of user-user cf\", mean_absolute_error(user_user_predictions[\"rating\"], user_user_predictions[\"predicted_rating\"]))\n",
    "print(\"MAPE of item-item cf\", mean_absolute_error(item_item_predictions[\"rating\"], item_item_predictions[\"predicted_rating\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that item-item performs a bit better than user-user, at least on this dataset.\n",
    "But to have a frame of reference, let's see what is the performance of a dumb predictor that just predicts the average rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"average_rating\"] = train_data[\"rating\"].mean()\n",
    "print(\"MAPE of average\", mean_absolute_error(test_data[\"rating\"], test_data[\"average_rating\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are indeed better than the baseline !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization\n",
    "Now we are going to try our hand with matrix factorization techniques, we are going to use the **surprise** package which just bundles together a bunch of algorithms in a easy to use manner, check it out [here](https://surprise.readthedocs.io/en/stable/getting_started.html)\n",
    "\n",
    "First we are going to build the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "import surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A reader is still needed but only the rating_scale param is requiered.\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "data = Dataset.load_from_df(ratings[['user_id', 'movie_id', 'rating']], reader)\n",
    "\n",
    "trainset, testset = surprise.model_selection.train_test_split(data, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we are going to use the SVD algorithm, and compute the MAE on the test data, here the train - test split is not the same as previously (for ease of use with the surprise package) but it still gives us an idea of how we perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the famous SVD algorithm.\n",
    "algo = SVD(n_factors=10, n_epochs=30, verbose=True)\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import accuracy\n",
    "\n",
    "accuracy.mae(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems it's better than our previous approaches ! Let's inspect the factors that we learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_factors = algo.qi\n",
    "item_factors_sim = 1 - pairwise_distances(item_factors, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_factors_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to redefine our function **recommendations_from_similarities** because of the specifics of the suprise package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_title = dict((row[\"movie_id\"], row[\"title\"]) for _, row in movies.iterrows())\n",
    "title_to_id = dict((row[\"title\"], row[\"movie_id\"]) for _, row in movies.iterrows())\n",
    "\n",
    "# Function that get movie recommendations based on the cosine similarity score of movie genres\n",
    "def recommendations_from_similarities_surprise(dataset, sims, title, k=10):\n",
    "    movie_id = title_to_id[title]\n",
    "    idx = dataset.to_inner_iid(movie_id)\n",
    "    sim_scores = list(enumerate(sims[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[:k+1]\n",
    "    # the most similar movie to a given one will always be itself, so we skip that one\n",
    "    movies = [id_to_title[dataset.to_raw_iid(i[0])] for i in sim_scores if i[0] != idx]\n",
    "    return movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_from_similarities_surprise(trainset, item_factors_sim, 'Matrix, The (1999)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_from_similarities_surprise(trainset, item_factors_sim, 'Toy Story (1995)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_from_similarities_surprise(trainset, item_factors_sim, 'Saving Private Ryan (1998)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
